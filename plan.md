# Implementation Plan: Hugind Native Server

**Context:**
*   **Existing Tools:** `hugind config` (generates YAML) and `hugind model` (manages GGUF paths).
*   **Core Library:** `llama_cpp_dart` v0.2 (provides `LlamaParent`, `LlamaScope`, `ContextParams`, etc.).
*   **Objective:** Build `hugind server`, an HTTP inference gateway that reads the configs and serves the models using the library's stateful slot mechanism.

---

### Phase 1: Configuration Mapping Layer
**Goal:** Bridge the gap between the YAML files generated by the CLI and the strictly typed parameters required by `llama_cpp_dart`.

1.  **Create `ServerConfig` Class:**
    *   **Input:** A parsed YAML map (from `config.yml`).
    *   **Fields:**
        *   `name`: String (The identifier used in OpenAI `model` field).
        *   `modelPath`: String (Absolute path to GGUF).
        *   `mmprojPath`: String? (Optional path to vision projector).
        *   `modelParams`: `ModelParams` (Mapped from YAML `gpu_layers`, `split_mode`, etc.).
        *   `contextParams`: `ContextParams` (Mapped from YAML `n_ctx`, `batch_size`, `threads`).
            *   *Instruction:* Ensure `typeK`, `typeV` (quantization) and `flashAttention` enums are mapped correctly from string values in YAML to library Enums.
        *   `samplerParams`: `SamplerParams` (Default sampling settings from YAML).
        *   `engineParams`: Class containing server specifics:
            *   `concurrency`: Int (Number of `LlamaParent` instances to spawn).
            *   `maxSlots`: Int (Max concurrent users per instance).
            *   `timeout`: Duration (Idle timeout for unloading).

2.  **Implement `ConfigLoader`:**
    *   Method `load(String configName)`:
        1.  Resolve path `~/.hugind/configs/{configName}.yml`.
        2.  Read YAML.
        3.  Validate file paths (Model exists?).
        4.  Return `ServerConfig`.

---

### Phase 2: The Engine Wrapper (`LlamaEngine`)
**Goal:** A wrapper around `LlamaParent` that manages the mapping between "User Session IDs" and "Library Scopes".

1.  **Create `LlamaEngine` Class:**
    *   **Dependencies:** `ServerConfig`.
    *   **State:**
        *   `_parent`: `LlamaParent` instance.
        *   `_sessions`: `Map<String, LlamaScope>` (Maps `user_id` -> `Scope`).
        *   `_lastUsed`: `Map<String, DateTime>` (For LRU eviction).
    *   **Lifecycle:**
        *   `init()`: Create `LlamaLoad` command from config, call `_parent.init()`.
        *   `dispose()`: Call `_parent.dispose()`.

2.  **Implement `generateStream` Method:**
    *   **Signature:** `Stream<String> generate(String userId, List<Message> messages)`
    *   **Logic:**
        1.  **Resolution:** Check `_sessions[userId]`.
            *   *Hit:* Use existing scope.
            *   *Miss:* Check active slots count.
                *   If `< maxSlots`: Call `_parent.getScope()`, save to map.
                *   If `>= maxSlots`: **Eviction Strategy**. Find oldest user in `_lastUsed`, call `scope.dispose()`, remove from map, create new scope.
        2.  **Formatting (The Stateful Trick):**
            *   Convert `messages` to `ChatHistory`.
            *   Call `chatHistory.getLatestTurn(format)`. **Do not** export full history.
            *   *Note:* If this is a new session (Miss), you might need to export full history *or* ensure the `ChatHistory` object provided contains the full context.
        3.  **Execution:**
            *   Call `scope.sendPrompt(formattedText)`.
            *   Update `_lastUsed[userId]`.
        4.  **Return:** The stream from the scope.

---

### Phase 3: The Orchestrator (`EngineManager`)
**Goal:** Manage multiple models and load balancing.

1.  **Create `EngineManager` Singleton:**
    *   **State:** `Map<String, List<LlamaEngine>> _deployments`.
        *   Key: Model Name (e.g., "gemma-2b").
        *   Value: List of engines (replicas).
    *   **Method `deploy(String configName)`:**
        *   Load config.
        *   Loop `config.concurrency` times:
            *   Instantiate `LlamaEngine`.
            *   `await engine.init()`.
            *   Add to list.
    *   **Method `routeRequest(String modelName, String userId)`:**
        *   Look up list for `modelName`.
        *   **Session Affinity:** Check if any engine in the list already has `userId` in its `_sessions` map. If yes, return that engine.
        *   **Load Balancing:** If no affinity, pick the engine with the fewest active slots.

---

### Phase 4: The HTTP Interface
**Goal:** OpenAI-compatible REST API.

1.  **Setup `shelf` or `shelf_router`:**
    *   Port 8080 (configurable).

2.  **Implement `POST /v1/chat/completions`:**
    *   **Parsing:**
        *   Extract `model` (Config name).
        *   Extract `messages` (Map to `List<Message>`).
        *   Extract `user` (This is the `userId`/SessionID). *Important: If null, generate a random temp ID.*
        *   Extract `stream` (bool).
    *   **Routing:**
        *   Call `EngineManager.instance.routeRequest(model, user)`.
    *   **Execution:**
        *   Call `engine.generate(user, messages)`.
    *   **Response (Streaming):**
        *   Set headers: `Content-Type: text/event-stream`.
        *   Listen to stream. For each token:
            *   Construct JSON: `{"id": "...", "choices": [{"delta": {"content": "token"}}], ...}`.
            *   Write `data: JSON\n\n`.
        *   On done: Write `data: [DONE]\n\n`.

3.  **Implement `GET /v1/models`:**
    *   Return list of keys from `EngineManager._deployments`.

---

### Phase 5: CLI Entry Point
**Goal:** Tie it to the `hugind` binary.

1.  **Update `bin/hugind.dart`:**
    *   Add command `server`.
    *   Subcommand `start`:
        *   Scans `~/.hugind/configs`.
        *   Optional flag `--autoload`: Automatically calls `deploy` for all found configs.
        *   Starts HTTP server.
        *   Keeps process alive.

---

### Key constraints for the AI Developer:
1.  **Do NOT** implement new Isolate logic. Use `LlamaParent` exclusively.
2.  **Do NOT** implement manual prompt formatting logic inside the handlers. Use `ChatHistory` and `PromptFormat` subclasses.
3.  **Must** handle the `LlamaKvCacheType` enum mapping carefully when reading the YAML (YAML strings "f16"/"q8_0" -> Enum values).


### File Structure
Here is the suggested file structure:

```text
lib
├── commands
│   ├── config_command.dart      # Existing
│   ├── config_templates.dart    # Existing
│   ├── model_command.dart       # Existing
│   └── server_command.dart      # NEW: 'hugind server start' entry point
├── repo_manager.dart            # Existing
└── server
    ├── api
    │   ├── dtos
    │   │   ├── openai_request.dart   # Maps JSON -> ChatHistory
    │   │   └── openai_response.dart  # Maps Stream -> SSE JSON format
    │   ├── handlers
    │   │   ├── chat_handler.dart     # POST /v1/chat/completions logic
    │   │   └── models_handler.dart   # GET /v1/models logic
    │   └── http_server.dart          # Shelf server setup & routing
    ├── config
    │   ├── config_loader.dart        # Reads YAML -> ServerConfig
    │   └── server_config.dart        # Typed config object (Phase 1)
    └── engine
        ├── engine_manager.dart       # The Load Balancer / Orchestrator (Phase 3)
        └── llama_engine.dart         # Wrapper around LlamaParent + Session Map (Phase 2)
```

### Detailed Breakdown by File

#### 1. `lib/commands/server_command.dart`
*   **Role:** CLI Entry point.
*   **Responsibility:** Parses arguments (`--port`, `--autoload`), initializes the `EngineManager`, and starts the `HttpServer`.

#### 2. `lib/server/config/` (Phase 1)
*   **`server_config.dart`**: Defines the `ServerConfig` class that holds `ModelParams`, `ContextParams`, and `EngineParams` (concurrency, slots).
*   **`config_loader.dart`**: Logic to read `~/.hugind/configs/*.yml`, resolve the GGUF path, and return a valid `ServerConfig`.

#### 3. `lib/server/engine/` (Phase 2 & 3)
*   **`llama_engine.dart`**: One instance per `LlamaParent` isolate.
    *   Holds the `Map<String, LlamaScope> _sessions`.
    *   Handles `getLatestTurn` formatting using your Utils.
*   **`engine_manager.dart`**: The Singleton.
    *   Holds `Map<String, List<LlamaEngine>>`.
    *   Handles logic for `routeRequest(model, user)`.
    *   Decides if a new slot needs to be created or if an old one needs eviction.

#### 4. `lib/server/api/` (Phase 4)
*   **`http_server.dart`**: Sets up `package:shelf` pipeline (logging, CORS) and defines the routes.
*   **`handlers/chat_handler.dart`**: The meat of the API. It accepts the request, calls the Engine, and transforms the Dart Stream into an HTTP Server-Sent Events (SSE) stream.
*   **`dtos/`**: Helper classes to parse the incoming OpenAI JSON body safely.

### Recommended Dependencies
You will likely need to add these to your `pubspec.yaml` for the server implementation:

```yaml
dependencies:
  shelf: ^1.4.0
  shelf_router: ^1.1.0
  shelf_cors: ^1.1.0 # Useful if calling from web browsers
  # json_annotation / build_runner (optional, but helpful for DTOs)
```